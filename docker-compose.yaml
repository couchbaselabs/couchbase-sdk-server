version: "3"
services:

  # Observability stack

  otel_integration_testing:
    image: otel/opentelemetry-collector:latest
    command: [ "--config=/etc/otel-collector.yaml" ]
    volumes:
      - ./observability/opentelemetry-collector-integration.yaml:/etc/otel-collector.yaml
    ports:
      - "10003:10003"   # OTLP GRPC
      - "10000:10000"   # Prometheus metrics (of what's sent to opentelemetry-collector)
      - "10006:8888"    # Prometheus metrics (of opentelemetry-collector's own metrics)

  otel_performance_testing:
    image: otel/opentelemetry-collector:latest
    command: [ "--config=/etc/otel-collector.yaml" ]
    volumes:
      - ./observability/opentelemetry-collector-performance.yaml:/etc/otel-collector.yaml
    ports:
      - "10001:10001"   # OTLP GRPC
      - "10002:10002"   # Prometheus metrics (of what's sent to opentelemetry-collector)
      - "10005:8888"    # Prometheus metrics (of opentelemetry-collector's own metrics)
      - "1888:1888"     # pprof extension
      - "13133:13133"   # health_check extension
      - "55679:55679"   # zpages extension

  otel_adhoc_testing:
    image: otel/opentelemetry-collector:latest
    command: [ "--config=/etc/otel-collector.yaml" ]
    volumes:
      - ./observability/opentelemetry-collector-adhoc.yaml:/etc/otel-collector.yaml
    ports:
      - "4317:4317"     # OTLP GRPC
      - "10004:10004"   # Prometheus metrics (of what's sent to opentelemetry-collector)
      - "10007:8888"    # Prometheus metrics (of opentelemetry-collector's own metrics)

  jaeger:
    image: jaegertracing/all-in-one:latest
    # Jaeger defaults to in-memory, and even with maximum traces set to 100, memory rapidly ballooned until the server
    # crashed.  Trying out Badger, a local storage backend, instead.  Traces are deleted after 72 hours by default.
    environment:
      SPAN_STORAGE_TYPE: badger
      BADGER_EPHEMERAL: false
      BADGER_DIRECTORY_VALUE: /badger/data
      BADGER_DIRECTORY_KEY: badger/key
    volumes:
      - /home/ec2-user/jaeger-data-badger:/badger
    ports:
      - "6831:6831/udp"
      - "16685:16685"  # grpc queries
      - "16686:16686"  # ui

#  tempo:
#    image: grafana/tempo:latest
#    command: [ "-config.file=/etc/tempo.yaml" ]
#    volumes:
#      - ./observability/tempo-local.yaml:/etc/tempo.yaml
#      - ./observability/tempo-data:/tmp/tempo
#    ports:
#      - "3200:3200"   # tempo
#    expose:
#      - "14268"  # jaeger ingest
#      - "4317"  # otlp grpc
#      - "4318"  # otlp http
#      - "9411"   # zipkin

  prometheus:
    image: prom/prometheus:latest
    command: [ "--config.file=/etc/prometheus.yaml" ]
    volumes:
      - ./observability/prometheus.yaml:/etc/prometheus.yaml
    ports:
      - "9090:9090"

#  grafana:
#    image: grafana/grafana:latest
#    volumes:
#      - ./observability/grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
#    environment:
#      - GF_AUTH_ANONYMOUS_ENABLED=true
#      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
#      - GF_AUTH_DISABLE_LOGIN_FORM=true
#    ports:
#      - "3000:3000"


  # Performance stack

  timedb:
    container_name: timedb
    image: timescale/timescaledb:latest-pg14
    volumes:
      - /home/ec2-user/perf-data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD
    ports:
      - "5432:5432"
    # The observability stack doesn't care what network it runs on, the performance stack expects 'perf'
    networks:
      - perf

networks:
  perf: